{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-28T20:32:54.391803Z",
     "start_time": "2019-12-28T20:32:54.382474Z"
    }
   },
   "outputs": [],
   "source": [
    "import twitter\n",
    "import spacy\n",
    "import nltk\n",
    "import gzip\n",
    "import re\n",
    "import json\n",
    "import itertools\n",
    "import functools\n",
    "import spacy_readability\n",
    "import string\n",
    "import collections\n",
    "import datetime\n",
    "import importlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T16:09:12.159463Z",
     "start_time": "2019-12-24T16:09:08.635113Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-26T22:05:53.418464Z",
     "start_time": "2019-12-26T22:05:53.399302Z"
    }
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import tree as skl_tree\n",
    "from sklearn import metrics as skl_metrics\n",
    "from sklearn import model_selection as skl_model_selection\n",
    "from sklearn import linear_model as skl_linear\n",
    "from sklearn import naive_bayes as skl_naive_bayes\n",
    "from sklearn import preprocessing as skl_preprocess\n",
    "from sklearn import feature_extraction as skl_ftex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T19:29:14.178950Z",
     "start_time": "2019-12-24T19:29:14.173529Z"
    }
   },
   "outputs": [],
   "source": [
    "#  List of twitter personalities to import\n",
    "TWITTER_HANDLES = [    \n",
    "    'jimmyfallon',\n",
    "    'trevornoah',\n",
    "    'billmaher',\n",
    "    'stephenathome',\n",
    "    'jimmyfallon',\n",
    "    'britneyspears',\n",
    "    'selenagomez',\n",
    "    'kimkardashian',\n",
    "    'jtimberlake',\n",
    "    'realdonaldtrump',\n",
    "    'arianagrande',\n",
    "    'theellenshow',\n",
    "    'ladygaga',\n",
    "    'taylorswift13',\n",
    "    'rihanna',\n",
    "    'justinbieber',\n",
    "    'katyperry',\n",
    "    'billgates',\n",
    "    'mileycyrus',\n",
    "    'jlo',\n",
    "    'kingjames',\n",
    "    'brunomars',\n",
    "    'chrissyteigen',\n",
    "    'oprah',\n",
    "    'drake',\n",
    "    'pink',\n",
    "    'liltunechi',\n",
    "    'kevinhart4real',\n",
    "    'elonmusk',\n",
    "    'kyliejenner',\n",
    "    'conanobrien',\n",
    "    'mariahcarey',\n",
    "    'davidguetta',\n",
    "    'jk_rowling'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T19:28:26.021488Z",
     "start_time": "2019-12-24T19:28:26.018362Z"
    }
   },
   "outputs": [],
   "source": [
    "#  Location to persist JSON data (so we don't need to re-load from API)\n",
    "SAVE_LOCATION = '/Users/liangjh/Workspace/tweemio-api/data/timelines'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T19:28:26.242296Z",
     "start_time": "2019-12-24T19:28:26.239319Z"
    }
   },
   "outputs": [],
   "source": [
    "#  Number of tweets to combine\n",
    "CONDENSE_FACTOR = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize NLP (Spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T19:35:08.880430Z",
     "start_time": "2019-12-24T19:35:08.568481Z"
    }
   },
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T19:35:09.865628Z",
     "start_time": "2019-12-24T19:35:08.884034Z"
    }
   },
   "outputs": [],
   "source": [
    "#  Initialize NLP parser / document processor\n",
    "#  Load small english dictionary, + readability metric pipeline\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#  Limit to pipeline we'll definitely need\n",
    "nlp.disable_pipes(*['tagger', 'parser', 'ner']) # all this stuff not needed\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'))  # we only need sentence parsing\n",
    "nlp.add_pipe(spacy_readability.Readability(), last=True)\n",
    "#  TODO: may remove NER from pipeline, if not used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter API Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T06:20:37.140616Z",
     "start_time": "2019-12-24T06:20:37.134955Z"
    }
   },
   "outputs": [],
   "source": [
    "twitter_credentials = {\n",
    "    'consumer_key':        'wbz78wFd0ywcShiTvqgDUV2ry',\n",
    "    'consumer_secret':     '2qj0P3fygqa0n2LqU6M8LV485OWIAvXWEQOEVLWFNUBdKDcgjz',\n",
    "    'access_token_key':    '80578720-t7bH4zwD6Q6sUQEFeCb8211wH04Y9ul0EWECo2ofU',\n",
    "    'access_token_secret': 'PSsX8R4agpxAII9XCYHqE74KObPRWfl9tdG4Xd07olOn6'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T06:20:37.279279Z",
     "start_time": "2019-12-24T06:20:37.273862Z"
    }
   },
   "outputs": [],
   "source": [
    "twapi = twitter.Api(consumer_key=twitter_credentials['consumer_key'],\n",
    "                    consumer_secret=twitter_credentials['consumer_secret'],\n",
    "                    access_token_key=twitter_credentials['access_token_key'],\n",
    "                    access_token_secret=twitter_credentials['access_token_secret'],\n",
    "                    tweet_mode='extended')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Timelines (Twitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T06:32:08.897324Z",
     "start_time": "2019-12-24T06:32:08.890944Z"
    }
   },
   "outputs": [],
   "source": [
    "def timeline_download(screen_name: str, twapi):\n",
    "    '''\n",
    "    Returns timeline for a given screen name\n",
    "    twitter.Api is expected to be initialized\n",
    "    '''\n",
    "    print('Retrieving timeline for: {}'.format(screen_name))\n",
    "    timeline = []\n",
    "    max_id = None\n",
    "    while True:        \n",
    "        print('     max_id for iteration: {}'.format(max_id))\n",
    "        tweets = twapi.GetUserTimeline(screen_name=screen_name, include_rts=False, count=200, max_id=max_id)\n",
    "        timeline += tweets\n",
    "\n",
    "        if len(tweets) < 1:\n",
    "            break        \n",
    "        next_max_id = min(tweets, key=lambda t: t.id).id\n",
    "        if next_max_id == max_id:\n",
    "            break\n",
    "        max_id = next_max_id\n",
    "    return timeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T06:42:18.939763Z",
     "start_time": "2019-12-24T06:42:15.484840Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  Download all, place into dict; \n",
    "#  Serialize to JSON, save w/ timestamp\n",
    "timeline_map = {}\n",
    "for screen_name in TWITTER_HANDLES:\n",
    "    user_timeline = timeline_download(screen_name, twapi)\n",
    "    timeline_map[screen_name] = user_timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T14:47:58.298357Z",
     "start_time": "2019-12-24T14:47:19.658835Z"
    }
   },
   "outputs": [],
   "source": [
    "for screen_name, tline in timeline_map.items():\n",
    "    print(f'writing {screen_name} to file...')\n",
    "    tline_js = [tli._json for tli in tline]\n",
    "    with open(\"{}/{}-{}.json\".format(SAVE_LOCATION, screen_name, datetime.date.today().strftime('%Y%m%d')), \"w\") as outfile:\n",
    "        json.dump(tline_js, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-28T20:30:43.993024Z",
     "start_time": "2019-12-28T20:30:41.350961Z"
    }
   },
   "outputs": [],
   "source": [
    "timeline_text = {screen_name: list(reversed([tli._json['full_text'] for tli in tline])) \n",
    "                 for screen_name, tline in timeline_map.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Saved Timelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T19:29:33.722706Z",
     "start_time": "2019-12-24T19:29:20.095912Z"
    }
   },
   "outputs": [],
   "source": [
    "#  All timeline JSON saved to file system (as of last retrieval)\n",
    "#  Load / parse for extraction\n",
    "timeline_map = {}\n",
    "for screen_name in TWITTER_HANDLES:\n",
    "    print(f'Loading {screen_name}')\n",
    "    fileloc = f'{SAVE_LOCATION}/{screen_name}-20191224.json'\n",
    "    with open(fileloc, 'r') as infile:\n",
    "        content = json.load(infile)\n",
    "        timeline_map[screen_name] = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T19:32:20.444627Z",
     "start_time": "2019-12-24T19:32:20.271052Z"
    }
   },
   "outputs": [],
   "source": [
    "timeline_text = {screen_name: list(reversed([tli['full_text'] for tli in tline])) \n",
    "                 for screen_name, tline in timeline_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-28T20:31:11.110961Z",
     "start_time": "2019-12-28T20:31:11.106310Z"
    }
   },
   "outputs": [],
   "source": [
    "timeline_text = {screen_name:tl[-400:] for screen_name,tl in timeline_text.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract, Construct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-28T20:31:22.189436Z",
     "start_time": "2019-12-28T20:31:22.178783Z"
    }
   },
   "outputs": [],
   "source": [
    "#  Remove non-parseable patterns ** adjustable **\n",
    "def timeline_text_construct(timeline_text: list, filter_regex: str='^(http)', condense_factor: int=1) -> list:\n",
    "    '''\n",
    "    Constructs timeline text based on filter regex as well as condense factor\n",
    "    (i.e. number of tweets to compress together)\n",
    "    '''\n",
    "    timeline_text = [' '.join([('' if (re.search(filter_regex, word) != None) else word) for word in text.split()]) for text in timeline_text]\n",
    "    timeline_text = [t for t in timeline_text if len(t.strip()) > 0]\n",
    "\n",
    "    #  Group by condense factor (i.e. grouping multiple tweets into single tweet)\n",
    "    timeline_text = [\n",
    "        ' '.join(timeline_group) for timeline_group in \n",
    "        zip(*[timeline_text[n::condense_factor] for n in range(0, condense_factor)])\n",
    "    ]\n",
    "    \n",
    "    return timeline_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-28T20:31:27.023359Z",
     "start_time": "2019-12-28T20:31:25.889308Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing jimmyfallon...\n",
      "Processing trevornoah...\n",
      "Processing billmaher...\n",
      "Processing stephenathome...\n",
      "Processing britneyspears...\n",
      "Processing selenagomez...\n",
      "Processing kimkardashian...\n",
      "Processing jtimberlake...\n",
      "Processing realdonaldtrump...\n",
      "Processing arianagrande...\n",
      "Processing theellenshow...\n",
      "Processing ladygaga...\n",
      "Processing taylorswift13...\n",
      "Processing rihanna...\n",
      "Processing justinbieber...\n",
      "Processing katyperry...\n",
      "Processing billgates...\n",
      "Processing mileycyrus...\n",
      "Processing jlo...\n",
      "Processing kingjames...\n",
      "Processing brunomars...\n",
      "Processing chrissyteigen...\n",
      "Processing oprah...\n",
      "Processing drake...\n",
      "Processing pink...\n",
      "Processing liltunechi...\n",
      "Processing kevinhart4real...\n",
      "Processing elonmusk...\n",
      "Processing kyliejenner...\n",
      "Processing conanobrien...\n",
      "Processing adele...\n",
      "Processing mariahcarey...\n",
      "Processing davidguetta...\n",
      "Processing jk_rowling...\n"
     ]
    }
   ],
   "source": [
    "#  Combine all timelines into a single dataframe, for feature extraction\n",
    "tl_dfs = []\n",
    "for screen_name, tline in timeline_text.items():\n",
    "    print(f'Processing {screen_name}...')\n",
    "\n",
    "    #  Parse tweet list: full text, nomention and mention-only\n",
    "    tline_text = timeline_text_construct(tline, '^(http)', CONDENSE_FACTOR)\n",
    "    tline_text_nomention = [' '.join([('' if (re.match(r'^(@|#|http)', word) != None) else word) for word in text.split()]).strip() for text in tline_text]    \n",
    "    tline_text_mention   = [' '.join([('' if (re.match(r'^(@|#)', word) == None) else word) for word in text.split()]).strip() for text in tline_text]\n",
    "    \n",
    "    tl_df = pd.DataFrame({'screenname': screen_name, 'text': tline_text, 'text_nomention': tline_text_nomention, 'text_mention': tline_text_mention})\n",
    "    tl_dfs.append(tl_df)\n",
    "\n",
    "feature_df = pd.concat(tl_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "// Lexical\n",
    "- number of words *per sentence*\n",
    "- number of words per \"tweet\"\n",
    "- number of sentences per \"tweet\" (or timeline element)\n",
    "- flesch kincaid grade level (via \n",
    "- flesch kincaid reading ease\n",
    "- dale chall\n",
    "- coleman liau\n",
    "- automated readability\n",
    "- ratio: number word extensions / number words in tweet\n",
    "- mimicry (overall in reference corpus) ** HARD\n",
    "\n",
    "// Syntactic\n",
    "- number of punctuations per sentence << treatment for multiple sentences? \n",
    "- ratio: number BOS characters capitalized / number sentences\n",
    "- ratio: number words w/ all caps / number of words\n",
    "- ratio: number alphanumeric words / number of words\n",
    "- number: special chars, digits, exclamation and question marks\n",
    "- ratio: number upper-case letters / number of letters\n",
    "- use quotation?  (1,0)\n",
    "\n",
    "// Tweet-specific\n",
    "- ratio: number hashtags / number of words\n",
    "- ratio: number @mentions / number of words\n",
    "- ratio: number urls / number of words\n",
    "- ratio: number emojis / number of words << if detection is possiblel ; note also utilize unicode)\n",
    "\n",
    "ON HOLD / ARCHIVE\n",
    "<< the below on hold, b/c applied to overall corpus\n",
    "* (TTR: types/tokens)\n",
    "* (hapax richness:  num single occurence types / total types)\n",
    "x - mimicry (consecutive, by word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T19:35:51.921314Z",
     "start_time": "2019-12-24T19:35:15.621531Z"
    }
   },
   "outputs": [],
   "source": [
    "#  Parse with spacy.io NLP parser\n",
    "feature_df['nlpdoc'] = feature_df.apply(lambda r: nlp(r['text']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T19:36:00.835481Z",
     "start_time": "2019-12-24T19:35:51.924735Z"
    }
   },
   "outputs": [],
   "source": [
    "#  Word / sentence counts (foundations)\n",
    "feature_df['words_per_sentence'] = feature_df.apply(lambda r: np.mean([len(s) for s in list(r['nlpdoc'].sents)]), axis=1)\n",
    "feature_df['words_per_segment'] = feature_df.apply(lambda r: len(r['text'].split()), axis=1)\n",
    "feature_df['sentences_per_segment'] = feature_df.apply(lambda r: len(list(r['nlpdoc'].sents)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T19:37:31.380539Z",
     "start_time": "2019-12-24T19:36:00.838199Z"
    }
   },
   "outputs": [],
   "source": [
    "#  Readability metrics / scores\n",
    "feature_df['re_fkgl']  = feature_df.apply(lambda r: r['nlpdoc']._.flesch_kincaid_grade_level, axis=1)\n",
    "feature_df['re_fkre']  = feature_df.apply(lambda r: r['nlpdoc']._.flesch_kincaid_reading_ease, axis=1)\n",
    "feature_df['re_dc']    = feature_df.apply(lambda r: r['nlpdoc']._.dale_chall, axis=1)\n",
    "feature_df['re_cl']    = feature_df.apply(lambda r: r['nlpdoc']._.coleman_liau_index, axis=1)\n",
    "feature_df['re_autor'] = feature_df.apply(lambda r: r['nlpdoc']._.automated_readability_index, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T19:37:57.250017Z",
     "start_time": "2019-12-24T19:37:31.384733Z"
    }
   },
   "outputs": [],
   "source": [
    "#  Matching word extensions\n",
    "feature_df['extensions_segment'] = feature_df.apply(\n",
    "    lambda r: len(re.findall(r'(\\w)\\1{2,}', r['text'])) / r['words_per_segment'], axis=1)\n",
    "#  Average punctuations per sentence\n",
    "feature_df['punc_per_sent'] = feature_df.apply(\n",
    "    lambda r: len(re.findall(r'(\\!|\\?|\\\"|\\'|\\\"|\\,|\\:|\\;|\\.)+', r['text'])) / r['sentences_per_segment'], axis=1)\n",
    "#  Cap BOS: sentences w/ first word capitalized / number sentences (avg)\n",
    "feature_df['bos_cap_per_sent'] = feature_df.apply(\n",
    "    lambda r: len([str(s[0])[0].isupper() for s in r['nlpdoc'].sents]) / len(list(r['nlpdoc'].sents)), axis=1)\n",
    "#  All caps: per segment\n",
    "feature_df['allcaps_per_segment'] = feature_df.apply(\n",
    "    lambda r: len([word for word in r['nlpdoc'] if str(word).isupper()]) / len(r['nlpdoc']), axis=1)\n",
    "#  Alphanumeric: words per segment (tweet)\n",
    "feature_df['alphanum_per_segment'] = feature_df.apply(\n",
    "    lambda r: len([word for word in r['nlpdoc'] if re.match(r'([0-9]+[a-zA-Z]+|[a-zA-Z]+[0-9]+)', str(word)) != None]) / r['words_per_segment'], axis=1)\n",
    "#  Upper-case letters per tweet (normalized to number of characters)\n",
    "feature_df['upcase_per_segment'] = feature_df.apply(\n",
    "    lambda r: len([char for word in r['nlpdoc'] for char in str(word) if char.isupper()]) / r['words_per_segment'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T19:37:59.895976Z",
     "start_time": "2019-12-24T19:37:57.253725Z"
    }
   },
   "outputs": [],
   "source": [
    "#  Mentions / hashtags as ratio of total tokens in segment\n",
    "feature_df['mention_ratio'] = feature_df.apply(lambda r: len([mention for mention in r['text_mention'] if mention.startswith('@')]) / r['words_per_segment'], axis=1)\n",
    "feature_df['hashtag_ratio'] = feature_df.apply(lambda r: len([mention for mention in r['text_mention'] if mention.startswith('#')]) / r['words_per_segment'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T19:38:00.599528Z",
     "start_time": "2019-12-24T19:37:59.899092Z"
    }
   },
   "outputs": [],
   "source": [
    "# If quotations are used, then will contain at least 3 elements even if entire thing is a quote\n",
    "feature_df['has_quotes'] = feature_df.apply(lambda r: len(r['text'].split('\\\"')) > 2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T19:38:00.708741Z",
     "start_time": "2019-12-24T19:38:00.602915Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words_per_sentence</th>\n",
       "      <th>words_per_segment</th>\n",
       "      <th>sentences_per_segment</th>\n",
       "      <th>re_fkgl</th>\n",
       "      <th>re_fkre</th>\n",
       "      <th>re_dc</th>\n",
       "      <th>re_cl</th>\n",
       "      <th>re_autor</th>\n",
       "      <th>extensions_segment</th>\n",
       "      <th>punc_per_sent</th>\n",
       "      <th>bos_cap_per_sent</th>\n",
       "      <th>allcaps_per_segment</th>\n",
       "      <th>alphanum_per_segment</th>\n",
       "      <th>upcase_per_segment</th>\n",
       "      <th>mention_ratio</th>\n",
       "      <th>hashtag_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>39044.000000</td>\n",
       "      <td>39044.000000</td>\n",
       "      <td>39044.000000</td>\n",
       "      <td>39044.000000</td>\n",
       "      <td>39044.000000</td>\n",
       "      <td>39044.000000</td>\n",
       "      <td>39044.000000</td>\n",
       "      <td>39044.000000</td>\n",
       "      <td>39044.000000</td>\n",
       "      <td>39044.000000</td>\n",
       "      <td>39044.0</td>\n",
       "      <td>39044.000000</td>\n",
       "      <td>39044.000000</td>\n",
       "      <td>39044.000000</td>\n",
       "      <td>39044.000000</td>\n",
       "      <td>39044.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>13.380761</td>\n",
       "      <td>26.934996</td>\n",
       "      <td>2.938300</td>\n",
       "      <td>5.974905</td>\n",
       "      <td>72.132607</td>\n",
       "      <td>10.206526</td>\n",
       "      <td>7.639619</td>\n",
       "      <td>5.941920</td>\n",
       "      <td>0.006634</td>\n",
       "      <td>1.416403</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.046589</td>\n",
       "      <td>0.006112</td>\n",
       "      <td>0.526119</td>\n",
       "      <td>0.057363</td>\n",
       "      <td>0.047323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.003326</td>\n",
       "      <td>16.134617</td>\n",
       "      <td>1.597166</td>\n",
       "      <td>5.116264</td>\n",
       "      <td>32.384771</td>\n",
       "      <td>3.001461</td>\n",
       "      <td>7.098094</td>\n",
       "      <td>6.324772</td>\n",
       "      <td>0.037683</td>\n",
       "      <td>1.022036</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.069018</td>\n",
       "      <td>0.029514</td>\n",
       "      <td>0.561689</td>\n",
       "      <td>0.090074</td>\n",
       "      <td>0.101476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-11.280000</td>\n",
       "      <td>-3075.498571</td>\n",
       "      <td>0.049600</td>\n",
       "      <td>-39.520000</td>\n",
       "      <td>-16.220000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.333333</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.216134</td>\n",
       "      <td>60.014257</td>\n",
       "      <td>8.153486</td>\n",
       "      <td>4.208710</td>\n",
       "      <td>2.407895</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>11.500000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.479613</td>\n",
       "      <td>75.142500</td>\n",
       "      <td>9.623803</td>\n",
       "      <td>7.249000</td>\n",
       "      <td>5.311935</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>16.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.113750</td>\n",
       "      <td>88.103026</td>\n",
       "      <td>11.663767</td>\n",
       "      <td>10.546857</td>\n",
       "      <td>8.752083</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.058824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>203.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>443.968571</td>\n",
       "      <td>184.840000</td>\n",
       "      <td>29.495300</td>\n",
       "      <td>443.651429</td>\n",
       "      <td>353.487143</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>23.833333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       words_per_sentence  words_per_segment  sentences_per_segment  \\\n",
       "count        39044.000000       39044.000000           39044.000000   \n",
       "mean            13.380761          26.934996               2.938300   \n",
       "std              8.003326          16.134617               1.597166   \n",
       "min              1.500000           2.000000               1.000000   \n",
       "25%              8.333333          15.000000               2.000000   \n",
       "50%             11.500000          25.000000               3.000000   \n",
       "75%             16.000000          35.000000               4.000000   \n",
       "max            203.000000         120.000000              18.000000   \n",
       "\n",
       "            re_fkgl       re_fkre         re_dc         re_cl      re_autor  \\\n",
       "count  39044.000000  39044.000000  39044.000000  39044.000000  39044.000000   \n",
       "mean       5.974905     72.132607     10.206526      7.639619      5.941920   \n",
       "std        5.116264     32.384771      3.001461      7.098094      6.324772   \n",
       "min      -11.280000  -3075.498571      0.049600    -39.520000    -16.220000   \n",
       "25%        3.216134     60.014257      8.153486      4.208710      2.407895   \n",
       "50%        5.479613     75.142500      9.623803      7.249000      5.311935   \n",
       "75%        8.113750     88.103026     11.663767     10.546857      8.752083   \n",
       "max      443.968571    184.840000     29.495300    443.651429    353.487143   \n",
       "\n",
       "       extensions_segment  punc_per_sent  bos_cap_per_sent  \\\n",
       "count        39044.000000   39044.000000           39044.0   \n",
       "mean             0.006634       1.416403               1.0   \n",
       "std              0.037683       1.022036               0.0   \n",
       "min              0.000000       0.000000               1.0   \n",
       "25%              0.000000       1.000000               1.0   \n",
       "50%              0.000000       1.250000               1.0   \n",
       "75%              0.000000       1.833333               1.0   \n",
       "max              1.666667      24.000000               1.0   \n",
       "\n",
       "       allcaps_per_segment  alphanum_per_segment  upcase_per_segment  \\\n",
       "count         39044.000000          39044.000000        39044.000000   \n",
       "mean              0.046589              0.006112            0.526119   \n",
       "std               0.069018              0.029514            0.561689   \n",
       "min               0.000000              0.000000            0.000000   \n",
       "25%               0.000000              0.000000            0.230769   \n",
       "50%               0.030303              0.000000            0.380952   \n",
       "75%               0.062500              0.000000            0.631579   \n",
       "max               1.000000              1.000000           23.833333   \n",
       "\n",
       "       mention_ratio  hashtag_ratio  \n",
       "count   39044.000000   39044.000000  \n",
       "mean        0.057363       0.047323  \n",
       "std         0.090074       0.101476  \n",
       "min         0.000000       0.000000  \n",
       "25%         0.000000       0.000000  \n",
       "50%         0.028571       0.000000  \n",
       "75%         0.076923       0.058824  \n",
       "max         1.000000       1.000000  "
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T15:14:30.857908Z",
     "start_time": "2019-12-24T15:14:29.486774Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_df.groupby('screenname').describe().to_csv('/Users/liangjh/Desktop/tweemio-summary.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Frequency Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequency of words, across the entire corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T22:06:19.828120Z",
     "start_time": "2019-12-24T22:06:19.808319Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['jimmyfallon', 'trevornoah', 'billmaher', 'stephenathome',\n",
       "       'britneyspears', 'selenagomez', 'kimkardashian', 'jtimberlake',\n",
       "       'realdonaldtrump', 'arianagrande', 'theellenshow', 'ladygaga',\n",
       "       'taylorswift13', 'rihanna', 'justinbieber', 'katyperry',\n",
       "       'billgates', 'mileycyrus', 'jlo', 'kingjames', 'brunomars',\n",
       "       'chrissyteigen', 'oprah', 'drake', 'pink', 'liltunechi',\n",
       "       'kevinhart4real', 'elonmusk', 'kyliejenner', 'conanobrien',\n",
       "       'adele', 'mariahcarey', 'davidguetta', 'jk_rowling'], dtype=object)"
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_df.screenname.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T22:08:09.445399Z",
     "start_time": "2019-12-24T22:08:09.328946Z"
    }
   },
   "outputs": [],
   "source": [
    "screen_name = 'realdonaldtrump'\n",
    "corpus = feature_df.loc[feature_df.screenname == screen_name, ['text_nomention']]\n",
    "corpus_joined = ' '.join(list(corpus.text_nomention))\n",
    "\n",
    "# Eliminate stop words and compound words (w/ contractions)\n",
    "corpus_word_counts = collections.Counter(\n",
    "            [word.lower() for word in corpus_joined.split() \n",
    "                if (word.lower() not in STOP_WORDS and re.search(r\"('|-|&|:|!|;)+\", word) == None)]\n",
    "        ).items()\n",
    "\n",
    "#  Get \"most frequent\" words for this user\n",
    "corpus_word_counts_sorted = sorted(corpus_word_counts, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist, Reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T16:08:14.687012Z",
     "start_time": "2019-12-24T16:08:14.398839Z"
    }
   },
   "outputs": [],
   "source": [
    "# feature_df[feature_df.columns.difference(['nlpdoc'])].to_pickle('/Users/liangjh/Desktop/feature_df.pik')\n",
    "# feature_df = pd.read_pickle('/Users/liangjh/Desktop/feature_df.pik')\n",
    "# feature_df[feature_df.columns.difference(['has_quotes', 'text' ,'text_mention', 'screenname'])].corr().to_csv('/Users/liangjh/Desktop/tweemio-corrs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPLORATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T18:37:37.765417Z",
     "start_time": "2019-12-24T18:37:37.762622Z"
    }
   },
   "outputs": [],
   "source": [
    "# feature_df.loc[feature_df.screenname == 'katyperry', ['re_autor', 're_cl', 're_dc', 're_fkgl', 're_fkre']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-25T16:20:30.040486Z",
     "start_time": "2019-12-25T16:20:30.030816Z"
    }
   },
   "outputs": [],
   "source": [
    "# list(feature_df.loc[feature_df.screenname == 'adele'].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T18:24:04.081082Z",
     "start_time": "2019-12-24T18:24:04.075370Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['allcaps_per_segment', 'alphanum_per_segment', 'bos_cap_per_sent',\n",
       "       'extensions_segment', 'has_quotes', 'hashtag_ratio', 'mention_ratio',\n",
       "       'punc_per_sent', 're_autor', 're_cl', 're_dc', 're_fkgl', 're_fkre',\n",
       "       'screenname', 'sentences_per_segment', 'text', 'text_mention',\n",
       "       'text_nomention', 'upcase_per_segment', 'words_per_segment',\n",
       "       'words_per_sentence'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T19:38:24.397459Z",
     "start_time": "2019-12-24T19:38:24.081511Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.1600e+02, 3.1400e+02, 8.9100e+02, 3.3220e+03, 1.2455e+04,\n",
       "        1.6994e+04, 4.1980e+03, 5.6100e+02, 3.7000e+01, 2.0000e+00]),\n",
       " array([-4. , -3.2, -2.4, -1.6, -0.8,  0. ,  0.8,  1.6,  2.4,  3.2,  4. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "feat = [[val] for val in feature_df.re_fkre]\n",
    "scaler = skl_preprocess.StandardScaler().fit(feat)\n",
    "scaled = scaler.transform(feat)\n",
    "plt.hist(scaled, range=(-4,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODELING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Logit: Using Extracted Variables"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "- logistic classification, all features\n",
    "- cross-validation\n",
    "- AUROC compilation\n",
    "- feature selection \n",
    "    - L1/L2 reduction\n",
    "    - entropy score (?)\n",
    "- random forest classification, all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T19:38:30.392933Z",
     "start_time": "2019-12-24T19:38:30.389323Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_cols = ['words_per_sentence', 'words_per_segment', 'sentences_per_segment',\n",
    "                'extensions_segment', 'punc_per_sent', 'bos_cap_per_sent',\n",
    "                'allcaps_per_segment', 'alphanum_per_segment', 'upcase_per_segment',\n",
    "                'mention_ratio', 'hashtag_ratio', 'has_quotes',\n",
    "                're_fkgl', 're_fkre', 're_dc', 're_cl', 're_autor']\n",
    "# feature_cols = ['re_fkgl', 're_fkre', 're_dc', 're_cl', 're_autor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-26T16:22:30.028037Z",
     "start_time": "2019-12-26T16:22:29.133002Z"
    }
   },
   "outputs": [],
   "source": [
    "# Binarize test\n",
    "# feature_df['sn'] = feature_df.apply(lambda r: 'realdonaldtrump' if r['screenname'] == 'realdonaldtrump' else 'NOT_realdonaldtrump', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-26T16:22:44.510791Z",
     "start_time": "2019-12-26T16:22:44.454607Z"
    }
   },
   "outputs": [],
   "source": [
    "x = feature_df.loc[:,  feature_cols].values\n",
    "y = feature_df.screenname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-26T16:22:45.746180Z",
     "start_time": "2019-12-26T16:22:45.728519Z"
    }
   },
   "outputs": [],
   "source": [
    "#  Training / test set separation\n",
    "x_train, x_test, y_train, y_test = skl_model_selection.train_test_split(x, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-26T16:22:46.780265Z",
     "start_time": "2019-12-26T16:22:46.763598Z"
    }
   },
   "outputs": [],
   "source": [
    "#  Scale all variables \n",
    "scaler  = skl_preprocess.StandardScaler()\n",
    "scaler  = scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test  = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-24T18:26:30.491143Z",
     "start_time": "2019-12-24T18:26:30.122087Z"
    }
   },
   "outputs": [],
   "source": [
    "# dtree_model = skl_tree.DecisionTreeClassifier().fit(x_train, y_train)\n",
    "# dtree_predictions = dtree_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-26T16:23:30.420238Z",
     "start_time": "2019-12-26T16:23:30.277820Z"
    }
   },
   "outputs": [],
   "source": [
    "logit_model = skl_linear.LogisticRegression(random_state=0, multi_class='multinomial', solver='newton-cg').fit(x_train, y_train)\n",
    "# logit_model = skl_linear.LogisticRegression(random_state=0).fit(x_train, y_train)\n",
    "logit_predictions = logit_model.predict(x_test)\n",
    "logit_predictions_prob = logit_model.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-26T16:24:01.804936Z",
     "start_time": "2019-12-26T16:24:01.766437Z"
    }
   },
   "outputs": [],
   "source": [
    "label_list = sorted(feature_df.sn.unique())\n",
    "cm = skl_metrics.confusion_matrix(y_test, logit_predictions, labels=label_list)\n",
    "cm_df = pd.DataFrame(cm, index=label_list, columns=label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-26T16:24:15.112770Z",
     "start_time": "2019-12-26T16:24:15.109083Z"
    }
   },
   "outputs": [],
   "source": [
    "# counts = collections.Counter(y_train)\n",
    "# sorted([(k,v) for k,v in counts.items()], key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# counts = collections.Counter(y_test)\n",
    "# sorted([(k,v) for k,v in counts.items()], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-26T16:24:16.469589Z",
     "start_time": "2019-12-26T16:24:16.456583Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NOT_realdonaldtrump</th>\n",
       "      <th>realdonaldtrump</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NOT_realdonaldtrump</th>\n",
       "      <td>9557</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>realdonaldtrump</th>\n",
       "      <td>203</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     NOT_realdonaldtrump  realdonaldtrump\n",
       "NOT_realdonaldtrump                 9557                1\n",
       "realdonaldtrump                      203                0"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pd.set_option('display.max_columns', None)\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic + Multinomial Bayes: Using N-Grams & Count Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct features based on word frequencies (1-gram, 2-gram), as sole input into predictive model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-28T20:32:28.357569Z",
     "start_time": "2019-12-28T20:32:28.339020Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_short_df = feature_df[['screenname', 'text', 'text_nomention', 'text_mention']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T06:19:23.135341Z",
     "start_time": "2019-12-29T06:19:01.605864Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating SN: jimmyfallon\n",
      "Evaluating SN: trevornoah\n",
      "Evaluating SN: billmaher\n",
      "Evaluating SN: stephenathome\n",
      "Evaluating SN: jimmyfallon\n",
      "Evaluating SN: britneyspears\n",
      "Evaluating SN: selenagomez\n",
      "Evaluating SN: kimkardashian\n",
      "Evaluating SN: jtimberlake\n",
      "Evaluating SN: realdonaldtrump\n",
      "Evaluating SN: arianagrande\n",
      "Evaluating SN: theellenshow\n",
      "Evaluating SN: ladygaga\n",
      "Evaluating SN: taylorswift13\n",
      "Evaluating SN: rihanna\n",
      "Evaluating SN: justinbieber\n",
      "Evaluating SN: katyperry\n",
      "Evaluating SN: billgates\n",
      "Evaluating SN: mileycyrus\n",
      "Evaluating SN: jlo\n",
      "Evaluating SN: kingjames\n",
      "Evaluating SN: brunomars\n",
      "Evaluating SN: chrissyteigen\n",
      "Evaluating SN: oprah\n",
      "Evaluating SN: drake\n",
      "Evaluating SN: pink\n",
      "Evaluating SN: liltunechi\n",
      "Evaluating SN: kevinhart4real\n",
      "Evaluating SN: elonmusk\n",
      "Evaluating SN: kyliejenner\n",
      "Evaluating SN: conanobrien\n",
      "Evaluating SN: adele\n",
      "Evaluating SN: mariahcarey\n",
      "Evaluating SN: davidguetta\n",
      "Evaluating SN: jk_rowling\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  Test accuracy of classification for each \n",
    "for screen_name in TWITTER_HANDLES:\n",
    "\n",
    "    print(f'Evaluating SN: {screen_name}')\n",
    "\n",
    "    #  y: label, x: texts (unsplit)\n",
    "    y = feature_short_df.apply(lambda r: screen_name if r['screenname'] == screen_name else f'NOT_{screen_name}', axis=1)\n",
    "    x = feature_short_df.text\n",
    "    \n",
    "#     x_train, x_test, y_train, y_test = skl_model_selection.train_test_split(x, y, random_state=1)\n",
    "    x_train = x\n",
    "    y_train = y\n",
    "\n",
    "    #  1-gram count vectorization\n",
    "    vect = skl_ftex.text.CountVectorizer().fit(x_train)\n",
    "    x_train_dtm = vect.transform(x_train)\n",
    "#     x_test_dtm = vect.transform(x_test)\n",
    "\n",
    "    #  Fit model \n",
    "    model_nb = skl_naive_bayes.MultinomialNB()\n",
    "    model_nb.fit(x_train_dtm, y_train)\n",
    "\n",
    "    pickle.dump(vect, open(f'/Users/liangjh/Workspace/tweemio-api/data/vects/{screen_name}.pik', 'wb'))\n",
    "    pickle.dump(model_nb, open(f'/Users/liangjh/Workspace/tweemio-api/data/models/{screen_name}.pik', 'wb'))\n",
    "    \n",
    "    '''\n",
    "    #  Prediction on testing set\n",
    "    y_pred_class_nb = model_nb.predict(x_test_dtm)\n",
    "    y_pred_prob_nb = model_nb.predict_proba(x_test_dtm)\n",
    "\n",
    "    #  Summary statistics\n",
    "    accuracy = skl_metrics.accuracy_score(y_test, y_pred_class_nb)\n",
    "    matching_indices = [i for i,x in enumerate(y_pred_class_nb)if x == screen_name]\n",
    "    matching_prob = [y_pred_prob_nb[idx][1] for idx in matching_indices]\n",
    "    stats = [np.mean(matching_prob), np.std(matching_prob), np.max(matching_prob), np.min(matching_prob)]\n",
    "\n",
    "    print(f'  Accuracy: {accuracy}')\n",
    "    print(f'  Stats(avg,std,max,min): {stats}')\n",
    "    '''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T22:33:23.478436Z",
     "start_time": "2019-12-29T22:33:23.363842Z"
    }
   },
   "outputs": [],
   "source": [
    "pickle.dump(vect.__dict__, open('/Users/liangjh/Desktop/out.pik', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T22:31:37.959371Z",
     "start_time": "2019-12-29T22:31:37.947667Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 1.0,\n",
       " 'fit_prior': True,\n",
       " 'class_prior': None,\n",
       " 'n_features_': 18939,\n",
       " 'classes_': array(['NOT_jk_rowling', 'jk_rowling'], dtype='<U14'),\n",
       " 'class_count_': array([6318.,  199.]),\n",
       " 'feature_count_': array([[ 6., 19.,  2., ...,  1.,  1.,  1.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]]),\n",
       " 'feature_log_prob_': array([[-10.25689215,  -9.20707003, -11.10419002, ..., -11.50965512,\n",
       "         -11.50965512, -11.50965512],\n",
       "        [-10.19723867, -10.19723867, -10.19723867, ..., -10.19723867,\n",
       "         -10.19723867, -10.19723867]]),\n",
       " 'class_log_prior_': array([-0.03101144, -3.4888646 ])}"
      ]
     },
     "execution_count": 741,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_nb.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T22:34:04.028850Z",
     "start_time": "2019-12-29T22:34:04.020597Z"
    }
   },
   "outputs": [],
   "source": [
    "pickle.dump(model_nb.__dict__, open('/Users/liangjh/Desktop/modelout.pik', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
