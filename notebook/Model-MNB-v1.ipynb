{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TWEEM.IO SIMILARITY MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: Multinomial Naive Bayes Text Classifier: v1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uses Multinomial Naive Bayes Classifier  \n",
    "Corpus is full list of tweets from group / category  \n",
    "Tweets can be downloaded via a different notebook, and persisted via a different notebook  \n",
    "Tweet library is vectorized via count vectorizer or td-idf  \n",
    "Use of NLP (spacy) to remove stop words, lemmatization, etc during training phases  \n",
    "Data persisted to local FS.  We can use local info  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:54:27.001472Z",
     "start_time": "2020-01-21T03:54:26.998173Z"
    }
   },
   "outputs": [],
   "source": [
    "#  Change to project base to allow assembly framework to init properly\n",
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T02:28:47.227184Z",
     "start_time": "2020-01-21T02:28:46.434973Z"
    }
   },
   "outputs": [],
   "source": [
    "import twitter\n",
    "import gzip\n",
    "import re\n",
    "import json\n",
    "import itertools\n",
    "import functools\n",
    "# import spacy\n",
    "# import spacy_readability\n",
    "import string\n",
    "import collections\n",
    "import datetime\n",
    "import importlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T02:29:10.197999Z",
     "start_time": "2020-01-21T02:29:10.193193Z"
    }
   },
   "outputs": [],
   "source": [
    "#  Scikit-learn imports\n",
    "import sklearn\n",
    "from sklearn import tree as skl_tree\n",
    "from sklearn import metrics as skl_metrics\n",
    "from sklearn import model_selection as skl_model_selection\n",
    "from sklearn import linear_model as skl_linear\n",
    "from sklearn import naive_bayes as skl_naive_bayes\n",
    "from sklearn import preprocessing as skl_preprocess\n",
    "from sklearn import feature_extraction as skl_ftex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T02:29:45.174114Z",
     "start_time": "2020-01-21T02:29:43.930357Z"
    }
   },
   "outputs": [],
   "source": [
    "import assembly\n",
    "from assembly import config as asmbl_config\n",
    "from assembly import db as asmbl_db\n",
    "from assembly import models as asmbl_models\n",
    "\n",
    "from lib.tweemio import twm\n",
    "from lib.tweemio import similarity\n",
    "from lib.tweemio import twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs / Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Settings controlling model calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T02:59:07.456992Z",
     "start_time": "2020-01-21T02:59:07.452626Z"
    }
   },
   "outputs": [],
   "source": [
    "group = 'default'  # name of group (default)\n",
    "condense_factor = 2   #  Tweets to combine into a single observation for model training purposes\n",
    "tline_recents = 400  #  How many recent tweets to evaluate in model\n",
    "tline_suffix = '20191224'  #  Last time tweet timeline was pulled \n",
    "tline_save_location = '/Users/liangjh/Workspace/tweemio-api/data/timelines'  # Timeline persistence location\n",
    "model_persist_dir = '/Users/liangjh/Workspace/tweemio-api/data/models'  #  Model persistence location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T02:52:33.723385Z",
     "start_time": "2020-01-21T02:52:33.618356Z"
    }
   },
   "outputs": [],
   "source": [
    "#  Assembly initialization - will use configs defined for env in question\n",
    "os.environ['ASSEMBLY_ENV'] = 'Development'\n",
    "app = assembly.Assembly.init(__name__, {'default': []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T02:53:29.065504Z",
     "start_time": "2020-01-21T02:53:29.061677Z"
    }
   },
   "outputs": [],
   "source": [
    "#  Twitter screen names to evaluate / calibrate (depends on the group)\n",
    "twitter_handles = app.config['SIMILARITY_COMPARISONS'][group]['screen_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T03:48:18.065992Z",
     "start_time": "2020-01-21T03:48:18.063273Z"
    }
   },
   "source": [
    "### Load Saved Tweet Timelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T02:59:18.874312Z",
     "start_time": "2020-01-21T02:59:08.634135Z"
    }
   },
   "outputs": [],
   "source": [
    "#  All timeline JSON saved to file system (as of last retrieval)\n",
    "#  Load / parse for extraction\n",
    "timeline_map = {}\n",
    "for screen_name in twitter_handles:\n",
    "#     print(f'Loading {screen_name}')\n",
    "    fileloc = f'{tline_save_location}/{screen_name}-{tline_suffix}.json'\n",
    "    with open(fileloc, 'r') as infile:\n",
    "        content = json.load(infile)\n",
    "        timeline_map[screen_name] = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeline_text = {screen_name: list(reversed([tli['full_text'] for tli in tline])) \n",
    "                 for screen_name, tline in timeline_map.items()}\n",
    "timeline_text = {screen_name:tl[-tline_recents:] for screen_name,tl in timeline_text.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Tweet Timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Remove non-parseable patterns ** adjustable **\n",
    "def timeline_text_construct(timeline_text: list, filter_regex: str='^(http)', condense_factor: int=1) -> list:\n",
    "    '''\n",
    "    Constructs timeline text based on filter regex as well as condense factor\n",
    "    (i.e. number of tweets to compress together)\n",
    "    '''\n",
    "    timeline_text = [' '.join([('' if (re.search(filter_regex, word) != None) else word) for word in text.split()]) for text in timeline_text]\n",
    "    timeline_text = [t for t in timeline_text if len(t.strip()) > 0]\n",
    "\n",
    "    #  Group by condense factor (i.e. grouping multiple tweets into single tweet)\n",
    "    timeline_text = [\n",
    "        ' '.join(timeline_group) for timeline_group in \n",
    "        zip(*[timeline_text[n::condense_factor] for n in range(0, condense_factor)])\n",
    "    ]\n",
    "    \n",
    "    return timeline_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Combine all timelines into a single dataframe, for feature extraction\n",
    "tl_dfs = []\n",
    "for screen_name, tline in timeline_text.items():\n",
    "    print(f'Processing {screen_name}...')\n",
    "\n",
    "    #  Parse tweet list: full text, nomention and mention-only\n",
    "    tline_text = timeline_text_construct(tline, '^(http)', condense_factor)\n",
    "    tline_text_nomention = [' '.join([('' if (re.match(r'^(@|#|http)', word) != None) else word) for word in text.split()]).strip() for text in tline_text]    \n",
    "    tline_text_mention   = [' '.join([('' if (re.match(r'^(@|#)', word) == None) else word) for word in text.split()]).strip() for text in tline_text]\n",
    "    \n",
    "    tl_df = pd.DataFrame({'screenname': screen_name, 'text': tline_text, 'text_nomention': tline_text_nomention, 'text_mention': tline_text_mention})\n",
    "    tl_dfs.append(tl_df)\n",
    "\n",
    "feature_df = pd.concat(tl_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Multinomial NB Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_short_df = feature_df[['screenname', 'text', 'text_nomention', 'text_mention']]\n",
    "\n",
    "#  As we are evaluating the full dataframe, we can calibrate the word vector once \n",
    "x = feature_short_df.text_nomention\n",
    "vect = skl_ftex.text.CountVectorizer().fit(x)\n",
    "\n",
    "#  Test accuracy of classification for each \n",
    "trained_models = {}\n",
    "for screen_name in twitter_handles:\n",
    "    print(f'Evaluating SN: {screen_name}')\n",
    "\n",
    "    #  For each screen name, its either in our out (binary)\n",
    "    y = feature_short_df.apply(lambda r: True if r['screenname'] == screen_name else False, axis=1)    \n",
    "    #  1-gram count vectorization on x\n",
    "    x_train_dtm = vect.transform(x_train)\n",
    "\n",
    "    #  Fit model \n",
    "    model_nb = skl_naive_bayes.MultinomialNB()\n",
    "    model_nb.fit(x_train_dtm, y_train)\n",
    "    trained_models[screen_name] = model_nb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persistence / Save Trained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to local file system.  \n",
    "Another notebook will handle saving to database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(vect, open(f'{model_persist_dir}/vectorizer.pik', 'wb'))\n",
    "for screen_name, model_nb in trained_models.items():\n",
    "    pickle.dump(model_nb, open(f'{model_persist_dir}/model-{screen_name}.pik', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
